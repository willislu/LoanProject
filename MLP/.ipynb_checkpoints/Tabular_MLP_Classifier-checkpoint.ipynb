{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5742ffe2",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorch Tabular Binary Classifier (MLP) — Loan Default (`bad_flag`)\n",
    "\n",
    "This notebook trains a **Multi-Layer Perceptron (MLP)** on your tabular dataset to predict `bad_flag` (binary target).\n",
    "\n",
    "> **Data assumption:** Rows with `bad_flag` = NaN are treated as **test** rows for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14240ee4",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Requirements\n",
    "Run this once to install dependencies (if needed):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c663dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally and you need packages, uncomment:\n",
    "#!pip install torch scikit-learn pandas joblib openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8a978",
   "metadata": {},
   "source": [
    "## 1) Imports & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38687697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os, json, random, math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5815e4",
   "metadata": {},
   "source": [
    "## 2) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "215ff338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291962, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>desc</th>\n",
       "      <th>...</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>application_approved_flag</th>\n",
       "      <th>internal_score</th>\n",
       "      <th>bad_flag</th>\n",
       "      <th>emp_length_num</th>\n",
       "      <th>desc_length</th>\n",
       "      <th>has_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10000001</td>\n",
       "      <td>11983056</td>\n",
       "      <td>7550</td>\n",
       "      <td>36</td>\n",
       "      <td>16.24</td>\n",
       "      <td>3 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3828.953801</td>\n",
       "      <td>5759.0</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10000002</td>\n",
       "      <td>12002921</td>\n",
       "      <td>27050</td>\n",
       "      <td>36</td>\n",
       "      <td>10.99</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>OWN</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>Borrower added on 12/31/13 &gt; Combining high ...</td>\n",
       "      <td>...</td>\n",
       "      <td>35700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34359.940730</td>\n",
       "      <td>114834.0</td>\n",
       "      <td>1</td>\n",
       "      <td>353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10000003</td>\n",
       "      <td>11983096</td>\n",
       "      <td>12000</td>\n",
       "      <td>36</td>\n",
       "      <td>10.99</td>\n",
       "      <td>4 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>Borrower added on 12/31/13 &gt; I would like to...</td>\n",
       "      <td>...</td>\n",
       "      <td>18100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16416.617760</td>\n",
       "      <td>7137.0</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10000004</td>\n",
       "      <td>12003142</td>\n",
       "      <td>28000</td>\n",
       "      <td>36</td>\n",
       "      <td>7.62</td>\n",
       "      <td>5 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>42200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38014.149760</td>\n",
       "      <td>799592.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10000005</td>\n",
       "      <td>11993233</td>\n",
       "      <td>12000</td>\n",
       "      <td>36</td>\n",
       "      <td>13.53</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6471.462236</td>\n",
       "      <td>13605.0</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        id  member_id  loan_amnt  term  int_rate emp_length  \\\n",
       "0           0  10000001   11983056       7550    36     16.24    3 years   \n",
       "1           1  10000002   12002921      27050    36     10.99  10+ years   \n",
       "2           2  10000003   11983096      12000    36     10.99    4 years   \n",
       "3           3  10000004   12003142      28000    36      7.62    5 years   \n",
       "4           4  10000005   11993233      12000    36     13.53  10+ years   \n",
       "\n",
       "  home_ownership  annual_inc  \\\n",
       "0           RENT     28000.0   \n",
       "1            OWN     55000.0   \n",
       "2           RENT     60000.0   \n",
       "3       MORTGAGE    325000.0   \n",
       "4           RENT     40000.0   \n",
       "\n",
       "                                                desc  ... total_bc_limit  \\\n",
       "0                                                NaN  ...         4000.0   \n",
       "1    Borrower added on 12/31/13 > Combining high ...  ...        35700.0   \n",
       "2    Borrower added on 12/31/13 > I would like to...  ...        18100.0   \n",
       "3                                                NaN  ...        42200.0   \n",
       "4                                                NaN  ...         7000.0   \n",
       "\n",
       "   mths_since_last_major_derog  tot_hi_cred_lim  tot_cur_bal  \\\n",
       "0                          NaN      3828.953801       5759.0   \n",
       "1                          NaN     34359.940730     114834.0   \n",
       "2                          NaN     16416.617760       7137.0   \n",
       "3                          NaN     38014.149760     799592.0   \n",
       "4                         53.0      6471.462236      13605.0   \n",
       "\n",
       "   application_approved_flag  internal_score  bad_flag  emp_length_num  \\\n",
       "0                          1              99       0.0             3.0   \n",
       "1                          1             353       0.0            10.0   \n",
       "2                          1             157       0.0             4.0   \n",
       "3                          1             365       0.0             5.0   \n",
       "4                          1             157       0.0            10.0   \n",
       "\n",
       "   desc_length  has_desc  \n",
       "0            0         0  \n",
       "1           95         1  \n",
       "2          176         1  \n",
       "3            0         0  \n",
       "4            0         0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Update the path if needed\n",
    "DATA_PATH = r\"C:\\Users\\luwil\\Documents\\misc_data\\modeldata.xlsx\"\n",
    "\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b11cb",
   "metadata": {},
   "source": [
    "## 3) Preprocessing Utilities - Data cleaning to drop unneeded columns, impute missing values, categorical one hot encoding, scaling numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8005c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DROP_COLS_DEFAULT = [\"Unnamed: 0\", \"member_id\", \"desc\"]   # obvious non-features\n",
    "POSSIBLE_DROP_IF_PRESENT = [\"emp_length\"]                 # keep emp_length_num instead if present\n",
    "\n",
    "def preprocess(df: pd.DataFrame, target_col: str = \"bad_flag\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop obvious columns if present\n",
    "    for c in DROP_COLS_DEFAULT + POSSIBLE_DROP_IF_PRESENT:\n",
    "        if c in df.columns:\n",
    "            df.drop(columns=[c], inplace=True)\n",
    "\n",
    "    # Identify test rows (bad_flag missing)\n",
    "    is_test = df[target_col].isna() if target_col in df.columns else pd.Series([False] * len(df))\n",
    "    train_df = df.loc[~is_test].copy()\n",
    "    test_df  = df.loc[is_test].copy()\n",
    "\n",
    "    # Target\n",
    "    y = None\n",
    "    if target_col in train_df.columns:\n",
    "        y = train_df[target_col].astype(int)\n",
    "        train_df.drop(columns=[target_col], inplace=True)\n",
    "    if target_col in test_df.columns:\n",
    "        test_df.drop(columns=[target_col], inplace=True)\n",
    "\n",
    "    # Separate types\n",
    "    cat_cols = train_df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = train_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "    # Keep ID for output but not as feature\n",
    "    id_col = None\n",
    "    for cand in [\"id\", \"ID\", \"Id\"]:\n",
    "        if cand in train_df.columns:\n",
    "            id_col = cand\n",
    "            if cand in num_cols: \n",
    "                num_cols.remove(cand)\n",
    "            break\n",
    "\n",
    "    # Impute missing\n",
    "    for c in cat_cols:\n",
    "        train_df[c] = train_df[c].fillna(\"Unknown\")\n",
    "    for c in num_cols:\n",
    "        med = train_df[c].median() # we use median here as it is robust to outliers.\n",
    "        train_df[c] = train_df[c].fillna(med)\n",
    "    # impute missing using the training portion of the dataset to prevent data leakage\n",
    "    if len(test_df) > 0:\n",
    "        for c in cat_cols:\n",
    "            if c in test_df.columns:\n",
    "                test_df[c] = test_df[c].fillna(\"Unknown\")\n",
    "        for c in num_cols:\n",
    "            if c in test_df.columns:\n",
    "                med = train_df[c].median()\n",
    "                test_df[c] = test_df[c].fillna(med)\n",
    "\n",
    "    # One-hot encode categoricals across combined frame to align columns\n",
    "    combined = pd.concat([train_df, test_df], axis=0, sort=False)\n",
    "    combined = pd.get_dummies(combined, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    # Split back\n",
    "    X_all   = combined\n",
    "    X_train = X_all.iloc[: len(train_df)].copy()\n",
    "    X_test  = X_all.iloc[len(train_df):].copy()\n",
    "\n",
    "    # Scale numeric columns only\n",
    "    scaler = StandardScaler()\n",
    "    if num_cols:\n",
    "        scaler.fit(X_train[num_cols].values)\n",
    "        X_train.loc[:, num_cols] = scaler.transform(X_train[num_cols].values)\n",
    "        if len(X_test) > 0:\n",
    "            X_test.loc[:, num_cols]  = scaler.transform(X_test[num_cols].values)\n",
    "\n",
    "    feature_cols = X_train.columns.tolist()\n",
    "    return X_train, y, X_test, feature_cols, scaler, id_col\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d84ec",
   "metadata": {},
   "source": [
    "## 4) Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "780eac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (189457, 36) Test shape: (102505, 36) Features: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:67: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.83870292  1.56511359 -0.29013967 ...  0.2029509  -1.02977552\n",
      "  1.6822226 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train.loc[:, num_cols] = scaler.transform(X_train[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:67: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.55534196 -0.55534196 -0.55534196 ...  1.80069231 -0.55534196\n",
      "  1.80069231]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train.loc[:, num_cols] = scaler.transform(X_train[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:67: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.77792947 -0.77792947  0.19018745 ... -0.77792947  0.19018745\n",
      "  1.15830436]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train.loc[:, num_cols] = scaler.transform(X_train[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:67: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.84053772  1.56832764 -0.29048185 ...  0.20267169 -1.03021216\n",
      "  1.6821323 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train.loc[:, num_cols] = scaler.transform(X_train[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:67: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.61905799  0.09496643  0.70376619 ...  1.44785479 -0.61905799\n",
      "  1.04198828]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train.loc[:, num_cols] = scaler.transform(X_train[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:67: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.87359119  1.14470019  1.14470019 ...  1.14470019 -0.87359119\n",
      "  1.14470019]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train.loc[:, num_cols] = scaler.transform(X_train[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.53668495 -1.59682967 -0.90650287 ... -1.52286608  0.78233232\n",
      " -0.53668495]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.loc[:, num_cols]  = scaler.transform(X_test[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.55534196 -0.55534196 -0.55534196 ... -0.55534196  1.80069231\n",
      " -0.55534196]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.loc[:, num_cols]  = scaler.transform(X_test[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.19018745 -0.77792947 -0.77792947 ...  0.19018745  1.15830436\n",
      " -0.77792947]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.loc[:, num_cols]  = scaler.transform(X_test[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.53705862 -1.59923547 -0.90692377 ... -1.5233657   0.78117872\n",
      " -0.53705862]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.loc[:, num_cols]  = scaler.transform(X_test[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.61905799 -0.61905799 -0.61905799 ... -0.61905799  1.17727712\n",
      "  0.14757875]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.loc[:, num_cols]  = scaler.transform(X_test[num_cols].values)\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_35092\\717782101.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.87359119 -0.87359119 -0.87359119 ... -0.87359119  1.14470019\n",
      "  1.14470019]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.loc[:, num_cols]  = scaler.transform(X_test[num_cols].values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bad_flag\n",
       "0    0.930707\n",
       "1    0.069293\n",
       "Name: class_ratio, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_df, y, X_test_df, feature_cols, scaler, id_col = preprocess(df, target_col=\"bad_flag\")\n",
    "print(\"Train shape:\", X_train_df.shape, \"Test shape:\", X_test_df.shape, \"Features:\", len(feature_cols))\n",
    "y.value_counts(normalize=True).rename(\"class_ratio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33676d89-2b35-4d3f-ace3-c4561ce3eee0",
   "metadata": {},
   "source": [
    "### define MLP class and dataset type class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad07adf7-818f-436c-90ce-cde1927b70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabular dataset is a function to make our dataset able to be used with pytorch's dataloader function\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray | None = None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: List[int], dropout: float = 0.2, use_bn: bool = True):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        prev = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))  # binary logit\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  # logits\n",
    "\n",
    "#%% md\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for Xb, yb in loader:\n",
    "        Xb = Xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * Xb.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    running_loss = 0.0\n",
    "    for Xb, yb in loader:\n",
    "        Xb = Xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        running_loss += loss.item() * Xb.size(0)\n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_targets.append(yb.detach().cpu().numpy())\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    logits = np.concatenate(all_logits)\n",
    "    targets = np.concatenate(all_targets).astype(int)\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(targets, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(targets, preds, average=\"binary\", zero_division=0)\n",
    "    return {\"loss\": avg_loss, \"auc\": auc, \"acc\": acc, \"precision\": pr, \"recall\": rc, \"f1\": f1}\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience: int = 5, mode: str = \"max\"):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.best = -float(\"inf\") if mode == \"max\" else float(\"inf\")\n",
    "        self.count = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metric_value, model):\n",
    "        improved = (metric_value > self.best) if self.mode == \"max\" else (metric_value < self.best)\n",
    "        if improved:\n",
    "            self.best = metric_value\n",
    "            self.count = 0\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return True\n",
    "        else:\n",
    "            self.count += 1\n",
    "            return False\n",
    "\n",
    "    def should_stop(self):\n",
    "        return self.count >= self.patience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012d1c9-fc17-427f-9be9-4155c7336ec6",
   "metadata": {},
   "source": [
    "### create train/validation split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bd14ce9-56e9-423e-bf9b-eb153241847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_df.astype(np.float32).values, \n",
    "    y.astype(np.float32).values,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y.values\n",
    ")\n",
    "\n",
    "train_ds = TabularDataset(X_train, y_train)\n",
    "val_ds   = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad622b6-4370-4644-899e-96b5399d3082",
   "metadata": {},
   "source": [
    "### train the model - iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fc4c82a-2615-4239-935c-d40a670c0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss 1.2296 | val_loss 2.7893 | AUC 0.6663 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 002 | loss 1.2249 | val_loss 18.7494 | AUC 0.6666 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 003 | loss 1.2232 | val_loss 2.1642 | AUC 0.6673 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 004 | loss 1.2222 | val_loss 1.6960 | AUC 0.6672 | acc 0.0693 | prec 0.0693 | rec 1.0000 | f1 0.1296\n",
      "Epoch 005 | loss 1.2207 | val_loss 4.3912 | AUC 0.6675 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 006 | loss 1.2203 | val_loss 1.5591 | AUC 0.6680 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 007 | loss 1.2208 | val_loss 2.3255 | AUC 0.6676 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 008 | loss 1.2189 | val_loss 1.3380 | AUC 0.6677 | acc 0.9229 | prec 0.1387 | rec 0.0217 | f1 0.0375\n",
      "Epoch 009 | loss 1.2199 | val_loss 1.3250 | AUC 0.6675 | acc 0.8403 | prec 0.1442 | rec 0.2643 | f1 0.1866\n",
      "Epoch 010 | loss 1.2199 | val_loss 1.3069 | AUC 0.6680 | acc 0.8715 | prec 0.1472 | rec 0.1782 | f1 0.1612\n",
      "Epoch 011 | loss 1.2189 | val_loss 5.8012 | AUC 0.6689 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 012 | loss 1.2181 | val_loss 10.3946 | AUC 0.6681 | acc 0.0693 | prec 0.0693 | rec 1.0000 | f1 0.1296\n",
      "Epoch 013 | loss 1.2189 | val_loss 1.4935 | AUC 0.6678 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 014 | loss 1.2187 | val_loss 1.4880 | AUC 0.6680 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 015 | loss 1.2187 | val_loss 1.2266 | AUC 0.6686 | acc 0.4844 | prec 0.0967 | rec 0.7715 | f1 0.1718\n",
      "Epoch 016 | loss 1.2187 | val_loss 2.8698 | AUC 0.6685 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 017 | loss 1.2184 | val_loss 3.2552 | AUC 0.6684 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Early stopping at epoch 17. Best epoch: 11 (AUC=0.6689)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, np.float64(0.6689239140815084))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = [128]     # starting with a fairly commonly used MLP architecture of two hidden layers\n",
    "dropout = 0.2 # regularization parameter\n",
    "lr = .0005 # learning rate\n",
    "weight_decay = .0001 #adds L2 regularization to prevent overfitting.\n",
    "epochs = 46 #max # of training iterations\n",
    "patience = 6 #early stopping parameter\n",
    "\n",
    "model = MLP(in_dim=X_train.shape[1], hidden=hidden, dropout=dropout, use_bn=True).to(device)\n",
    "\n",
    "# class imbalance handling\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "pos_weight = torch.tensor([neg / max(pos, 1)], dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "early = EarlyStopper(patience=patience, mode=\"max\")\n",
    "\n",
    "best_epoch = -1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | loss {tr_loss:.4f} | val_loss {val_metrics['loss']:.4f} | \"\n",
    "          f\"AUC {val_metrics['auc']:.4f} | acc {val_metrics['acc']:.4f} | \"\n",
    "          f\"prec {val_metrics['precision']:.4f} | rec {val_metrics['recall']:.4f} | f1 {val_metrics['f1']:.4f}\")\n",
    "\n",
    "    improved = early.step(val_metrics[\"auc\"], model)\n",
    "    if improved:\n",
    "        best_epoch = epoch\n",
    "    if early.should_stop():\n",
    "        print(f\"Early stopping at epoch {epoch}. Best epoch: {best_epoch} (AUC={early.best:.4f})\")\n",
    "        break\n",
    "\n",
    "# Restore best weights\n",
    "if early.best_state is not None:\n",
    "    model.load_state_dict(early.best_state)\n",
    "\n",
    "best_epoch, early.best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d795c-6937-4eed-96c4-ddfdfdec5c11",
   "metadata": {},
   "source": [
    "### grid search cross validation - instead of grid search CV on just 1 train/val split we could do a k-fold cross validation instead which has many splits and makes our model fitting more consistent and stable. However, it takes a lot more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a2d385e-ba01-47f4-8972-325ab3759ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6691 @ epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 2\n",
      "Tried hidden=[128], drop=0.2, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6711 @ epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.7167 @ epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.2, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6730 @ epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.2, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6677 @ epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.2, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6681 @ epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.2, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6675 @ epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.2, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6675 @ epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.2, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.3, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6681 @ epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.3, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.3, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6706 @ epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.3, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.3, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.6682 @ epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.3, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.3, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6677 @ epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.3, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.3, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6772 @ epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.3, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.3, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6671 @ epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.3, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 2\n",
      "Tried hidden=[128], drop=0.3, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6670 @ epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.3, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.3, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6673 @ epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.3, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.4, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6673 @ epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.4, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.4, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6673 @ epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.4, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.4, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.6672 @ epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.4, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.4, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6669 @ epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.4, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.4, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6670 @ epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.4, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.6664 @ epoch 8\n",
      "Tried hidden=[128], drop=0.4, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6666 @ epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.4, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.4, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6670 @ epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.4, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128], drop=0.4, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6666 @ epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n",
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128], drop=0.4, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6826 @ epoch 25\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6676 @ epoch 6\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 3\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.6690 @ epoch 10\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.6660 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6689 @ epoch 20\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128, 64], drop=0.2, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6719 @ epoch 21\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6667 @ epoch 6\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 2\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6764 @ epoch 20\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luwil\\AppData\\Local\\Temp\\ipykernel_19588\\991602953.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried hidden=[128, 64], drop=0.2, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6935 @ epoch 45\n",
      "Tried hidden=[128, 64], drop=0.2, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6671 @ epoch 5\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6670 @ epoch 8\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.6661 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.6695 @ epoch 11\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6691 @ epoch 24\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6941 @ epoch 39\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6681 @ epoch 28\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.6660 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6685 @ epoch 18\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6675 @ epoch 19\n",
      "Tried hidden=[128, 64], drop=0.3, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6671 @ epoch 9\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6670 @ epoch 13\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 3\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.6673 @ epoch 5\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 2\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6670 @ epoch 10\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6677 @ epoch 24\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6673 @ epoch 25\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6669 @ epoch 13\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6666 @ epoch 13\n",
      "Tried hidden=[128, 64], drop=0.4, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6894 @ epoch 14\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6752 @ epoch 11\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.7001 @ epoch 35\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6746 @ epoch 24\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.6661 @ epoch 2\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6823 @ epoch 15\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 2\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6684 @ epoch 3\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6866 @ epoch 21\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6668 @ epoch 3\n",
      "Tried hidden=[256, 128, 64], drop=0.2, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6673 @ epoch 4\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6676 @ epoch 13\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.6661 @ epoch 2\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.7025 @ epoch 26\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6714 @ epoch 23\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6776 @ epoch 14\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6684 @ epoch 17\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6772 @ epoch 9\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6683 @ epoch 6\n",
      "Tried hidden=[256, 128, 64], drop=0.3, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.0005, wd=1e-05, bs=256, bn=True -> AUC=0.6676 @ epoch 9\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.0005, wd=1e-05, bs=256, bn=False -> AUC=0.6661 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.0005, wd=1e-05, bs=512, bn=True -> AUC=0.6671 @ epoch 9\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.0005, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.0005, wd=0.0001, bs=256, bn=True -> AUC=0.6705 @ epoch 16\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.0005, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.0005, wd=0.0001, bs=512, bn=True -> AUC=0.6706 @ epoch 23\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.0005, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.001, wd=1e-05, bs=256, bn=True -> AUC=0.6710 @ epoch 19\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.001, wd=1e-05, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.001, wd=1e-05, bs=512, bn=True -> AUC=0.6691 @ epoch 8\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.001, wd=1e-05, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.001, wd=0.0001, bs=256, bn=True -> AUC=0.6716 @ epoch 16\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.001, wd=0.0001, bs=256, bn=False -> AUC=0.5000 @ epoch 1\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.001, wd=0.0001, bs=512, bn=True -> AUC=0.6669 @ epoch 5\n",
      "Tried hidden=[256, 128, 64], drop=0.4, lr=0.001, wd=0.0001, bs=512, bn=False -> AUC=0.5000 @ epoch 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hidden': [128],\n",
       " 'dropout': 0.2,\n",
       " 'lr': 0.0005,\n",
       " 'weight_decay': 0.0001,\n",
       " 'batch_size': 256,\n",
       " 'use_bn': True,\n",
       " 'best_auc': np.float64(0.7166818492156812),\n",
       " 'best_epoch': 46}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def run_once(hidden, dropout, lr, weight_decay, batch_size, use_bn=True, epochs=50, patience=6):\n",
    "    # Datasets/loaders (rebuild per batch size)\n",
    "    train_ds = TabularDataset(X_train, y_train.astype(np.float32))\n",
    "    val_ds   = TabularDataset(X_val,   y_val.astype(np.float32))\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    model = MLP(in_dim=X_train.shape[1], hidden=hidden, dropout=dropout, use_bn=use_bn).to(device)\n",
    "\n",
    "    # Class imbalance\n",
    "    pos = (y_train == 1).sum()\n",
    "    neg = (y_train == 0).sum()\n",
    "    pos_weight = torch.tensor([neg / max(pos, 1)], dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    early = EarlyStopper(patience=patience, mode=\"max\")\n",
    "\n",
    "    best_epoch = -1\n",
    "    best_auc = float(\"-inf\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        auc = val_metrics[\"auc\"]\n",
    "        if early.step(auc, model):\n",
    "            best_epoch = epoch\n",
    "            best_auc = auc\n",
    "        if early.should_stop():\n",
    "            break\n",
    "\n",
    "    # restore best weights\n",
    "    if early.best_state is not None:\n",
    "        model.load_state_dict(early.best_state)\n",
    "\n",
    "    return best_auc, best_epoch, model\n",
    "\n",
    "# ------- Define search space -------\n",
    "param_grid = {\n",
    "    \"hidden\":       [[128], [128,64], [256,128,64]],  # [] = logistic regression baseline\n",
    "    \"dropout\":      [0.2, 0.3, 0.4],\n",
    "    \"lr\":           [5e-4, 1e-3],\n",
    "    \"weight_decay\": [1e-5, 1e-4],\n",
    "    \"batch_size\":   [256, 512],\n",
    "    \"use_bn\":       [True, False],\n",
    "}\n",
    "\n",
    "results = []\n",
    "for hidden, dropout, lr, weight_decay, batch_size, use_bn in itertools.product(\n",
    "    param_grid[\"hidden\"],\n",
    "    param_grid[\"dropout\"],\n",
    "    param_grid[\"lr\"],\n",
    "    param_grid[\"weight_decay\"],\n",
    "    param_grid[\"batch_size\"],\n",
    "    param_grid[\"use_bn\"]\n",
    "):\n",
    "    auc, best_ep, _ = run_once(hidden, dropout, lr, weight_decay, batch_size, use_bn=use_bn, epochs=50, patience=6)\n",
    "    results.append({\n",
    "        \"hidden\": hidden, \"dropout\": dropout, \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay, \"batch_size\": batch_size, \"use_bn\": use_bn,\n",
    "        \"best_auc\": auc, \"best_epoch\": best_ep\n",
    "    })\n",
    "    print(f\"Tried hidden={hidden}, drop={dropout}, lr={lr}, wd={weight_decay}, bs={batch_size}, bn={use_bn} -> AUC={auc:.4f} @ epoch {best_ep}\")\n",
    "\n",
    "# Pick best\n",
    "results_sorted = sorted(results, key=lambda d: d[\"best_auc\"], reverse=True)\n",
    "best = results_sorted[0]\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8a323-4398-4726-ae94-00054d7bf948",
   "metadata": {},
   "source": [
    "### We try several combinations of each hyperparameter. I belive the most important one we are tuning here is the number of hidden layers. With more hidden layers being able to detect more subtle relationships. Less hidden layers being less prone to overfitting but may miss those subtle relationships.\n",
    "Below is the final model with hyperparameters set from the best AUC model from grid search cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dc3461d-b8ee-423c-9722-9d3390e8a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss 1.2262 | val_loss 2.0119 | AUC 0.6667 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 002 | loss 1.2244 | val_loss 5.2809 | AUC 0.6679 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 003 | loss 1.2236 | val_loss 3.0926 | AUC 0.6675 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 004 | loss 1.2213 | val_loss 1.3971 | AUC 0.6677 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 005 | loss 1.2212 | val_loss 2.9499 | AUC 0.6675 | acc 0.0693 | prec 0.0693 | rec 1.0000 | f1 0.1296\n",
      "Epoch 006 | loss 1.2203 | val_loss 4.9679 | AUC 0.6675 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Epoch 007 | loss 1.2217 | val_loss 1.4119 | AUC 0.6676 | acc 0.9302 | prec 0.1600 | rec 0.0015 | f1 0.0030\n",
      "Epoch 008 | loss 1.2199 | val_loss 2.2833 | AUC 0.6678 | acc 0.9307 | prec 0.0000 | rec 0.0000 | f1 0.0000\n",
      "Early stopping at epoch 8. Best epoch: 2 (AUC=0.6679)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, np.float64(0.6678843066657066))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- hyperparams ---\n",
    "hidden = [128]          # one hidden layer (128 units)\n",
    "dropout = 0.2           # regularization parameter\n",
    "lr = 5e-4               # learning rate\n",
    "weight_decay = 1e-4     # L2 regularization\n",
    "epochs = 46             # max training epochs\n",
    "patience = 6            # early stopping patience\n",
    "batch_size = 256        # << set batch size to 256\n",
    "use_bn = True           # << enable BatchNorm\n",
    "\n",
    "# --- (re)build DataLoaders with batch_size=256 ---\n",
    "from torch.utils.data import DataLoader\n",
    "train_ds = TabularDataset(X_train.astype(np.float32), y_train.astype(np.float32))\n",
    "val_ds   = TabularDataset(X_val.astype(np.float32),   y_val.astype(np.float32))\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- model / loss / optim ---\n",
    "model = MLP(in_dim=X_train.shape[1], hidden=hidden, dropout=dropout, use_bn=use_bn).to(device)\n",
    "\n",
    "# class imbalance handling\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "pos_weight = torch.tensor([neg / max(pos, 1)], dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "early = EarlyStopper(patience=patience, mode=\"max\")\n",
    "\n",
    "# --- training loop ---\n",
    "best_epoch = -1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | loss {tr_loss:.4f} | val_loss {val_metrics['loss']:.4f} | \"\n",
    "          f\"AUC {val_metrics['auc']:.4f} | acc {val_metrics['acc']:.4f} | \"\n",
    "          f\"prec {val_metrics['precision']:.4f} | rec {val_metrics['recall']:.4f} | f1 {val_metrics['f1']:.4f}\")\n",
    "\n",
    "    if early.step(val_metrics[\"auc\"], model):\n",
    "        best_epoch = epoch\n",
    "    if early.should_stop():\n",
    "        print(f\"Early stopping at epoch {epoch}. Best epoch: {best_epoch} (AUC={early.best:.4f})\")\n",
    "        break\n",
    "\n",
    "# restore best weights\n",
    "if early.best_state is not None:\n",
    "    model.load_state_dict(early.best_state)\n",
    "\n",
    "best_epoch, early.best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bd545-afea-442b-a129-76ecb33d9d9a",
   "metadata": {},
   "source": [
    "### Our AUC is .667 which is around what we got from iteration 1. Grid search CV had previously shown best AUC around .71. The model doesn't seem very good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b835233-8289-43b2-a28d-cae332430ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\luwil\\\\Documents\\\\LoanProject\\\\MLP\\\\nn_model.pt',\n",
       " 'C:\\\\Users\\\\luwil\\\\Documents\\\\LoanProject\\\\MLP\\\\nn_features.json',\n",
       " 'C:\\\\Users\\\\luwil\\\\Documents\\\\LoanProject\\\\MLP\\\\nn_scaler.pkl')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ARTIF_DIR = Path(r\"C:\\Users\\luwil\\Documents\\LoanProject\\MLP\")  # change if you prefer\n",
    "ARTIF_DIR.mkdir(parents=True, exist_ok=True)  # <-- creates folder if missing\n",
    "\n",
    "model_path  = os.path.join(ARTIF_DIR, \"nn_model.pt\")\n",
    "feats_path  = os.path.join(ARTIF_DIR, \"nn_features.json\")\n",
    "scaler_path = os.path.join(ARTIF_DIR, \"nn_scaler.pkl\")\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "with open(feats_path, \"w\") as f:\n",
    "    json.dump({\"feature_cols\": feature_cols}, f, indent=2)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "model_path, feats_path, scaler_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e539943-e90e-45c8-a3ec-16c0143e24fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m pred_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ARTIF_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn_predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_test_df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m     test_ds \u001b[38;5;241m=\u001b[39m TabularDataset(X_test_df\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      5\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m, in \u001b[0;36mTabularDataset.__init__\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: np\u001b[38;5;241m.\u001b[39mndarray, y: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "pred_path = os.path.join(ARTIF_DIR, \"nn_predictions.csv\")\n",
    "\n",
    "if len(X_test_df) > 0:\n",
    "    X_np = (\n",
    "    X_test_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "             .fillna(0.0)              # or another fill policy\n",
    "             .to_numpy(dtype=np.float32)\n",
    "    )\n",
    "    \n",
    "    test_ds = TabularDataset(X_np)\n",
    "    test_ds = TabularDataset(X_test_df.values)\n",
    "    test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for Xb in test_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            logits = model(Xb)\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-np.concatenate(all_logits)))\n",
    "    out = pd.DataFrame({\"bad_flag_pred_prob\": probs})\n",
    "    if \"bad_flag\" in df.columns:\n",
    "        mask = df[\"bad_flag\"].isna()\n",
    "        if 'id' in df.columns:\n",
    "            out['id'] = df.loc[mask, 'id'].values\n",
    "            out = out[['id', 'bad_flag_pred_prob']]\n",
    "    out.to_csv(pred_path, index=False)\n",
    "    print(f\"Wrote predictions to {pred_path}\")\n",
    "else:\n",
    "    print(\"No test rows detected (no NaN target rows). Skipping inference.\")\n",
    "\n",
    "pred_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26c15ff7-7ea8-4be2-b837-0bf0339dc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load artifacts ---\n",
    "with open(feats_path, \"r\") as f:\n",
    "    feature_cols = json.load(f)[\"feature_cols\"]\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "if hasattr(scaler, \"feature_names_in_\"):\n",
    "    expected_cols = list(scaler.feature_names_in_)\n",
    "else:\n",
    "    # fall back to your saved list if scaler doesn't have names\n",
    "    expected_cols = list(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ec6f700-abac-4f05-aad1-ed8c55122399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'loan_amnt',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'annual_inc',\n",
       " 'percent_bc_gt_75',\n",
       " 'bc_util',\n",
       " 'dti',\n",
       " 'inq_last_6mths',\n",
       " 'mths_since_recent_inq',\n",
       " 'revol_util',\n",
       " 'total_bc_limit',\n",
       " 'mths_since_last_major_derog',\n",
       " 'tot_hi_cred_lim',\n",
       " 'tot_cur_bal',\n",
       " 'application_approved_flag',\n",
       " 'internal_score',\n",
       " 'emp_length_num',\n",
       " 'desc_length',\n",
       " 'has_desc',\n",
       " 'home_ownership_NONE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'purpose_credit_card',\n",
       " 'purpose_debt_consolidation',\n",
       " 'purpose_home_improvement',\n",
       " 'purpose_house',\n",
       " 'purpose_major_purchase',\n",
       " 'purpose_medical',\n",
       " 'purpose_moving',\n",
       " 'purpose_other',\n",
       " 'purpose_renewable_energy',\n",
       " 'purpose_small_business',\n",
       " 'purpose_vacation',\n",
       " 'purpose_wedding']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bf974-e6cd-4310-bf7f-780c0ea97754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
